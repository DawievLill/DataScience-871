<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Data Science for Economics</title>
    <meta charset="utf-8" />
    <meta name="author" content="Dawie van Lill" />
    <script src="libs/header-attrs-2.7/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Data Science for Economics
## Lecture 2: Shrinkage methods
### Dawie van Lill
### 2021/04/05

---




## Packages

I thought slides would be easier to present than notes. 

The code and slides are on the Github page. 

Here are the packages that we will be using for this session.  


```r
if (!require("pacman")) install.packages("pacman")
pacman::p_load(dplyr, ggplot2, rsample, caret, glmnet, vip, tidyverse, AmesHousing)
```

Let us give quick rundown of some of the potentially new packages.

**rsample**:

**caret**: Machine learning

**glmnet**:

---

## Linear regression (one variable)
 
Quick example of linear regression using machine learning process. 

Want to model linear relationship between total above ground living space of a home (`Gr_Liv_Area`) and sale price (`Sale_Price`). 

First, we construct our training sample.  


```r
set.seed(123)
ames &lt;- AmesHousing::make_ames()
split  &lt;- initial_split(ames, prop = 0.7, strata = "Sale_Price")
ames_train  &lt;- training(split)
ames_test   &lt;- testing(split)
```

Second, we model on training data.  


```r
model1 &lt;- lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
```

In the next slide we show the results from this regression. 

---

&lt;img src="lecture-2-slides_files/figure-html/regression_1-1.png" width="720" style="display: block; margin: auto;" /&gt;
---

&lt;img src="lecture-2-slides_files/figure-html/regression_2-1.png" width="720" style="display: block; margin: auto;" /&gt;
---

## Evaluate results


```r
summary(model1)
```

```
## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -361143  -30668   -2449   22838  331357 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 8732.938   3996.613   2.185    0.029 *  
## Gr_Liv_Area  114.876      2.531  45.385   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 56700 on 2051 degrees of freedom
## Multiple R-squared:  0.5011,	Adjusted R-squared:  0.5008 
## F-statistic:  2060 on 1 and 2051 DF,  p-value: &lt; 2.2e-16
```

---

## Multiple linear regression

You can include more than one predictor, such as above ground square footage and year house was built. 


```r
model2 &lt;- lm(Sale_Price ~ Gr_Liv_Area + Year_Built, data = ames_train)
```

One could add as many predictors as you want. 

```r
model3 &lt;- lm(Sale_Price ~ ., data = ames_train) 
```

Let us now test the results from the models to see which one is the most accurate. 
---

## Linear regression


```r
set.seed(123)  
(cv_model1 &lt;- train(form = Sale_Price ~ Gr_Liv_Area, 
  data = ames_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))
```

```
## Linear Regression 
## 
## 2053 samples
##    1 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1846, 1848, 1848, 1848, 1848, 1848, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   56410.89  0.5069425  39169.09
## 
## Tuning parameter 'intercept' was held constant at a value of TRUE
```

---

## Multiple linear regression


```r
set.seed(123)  
(cv_model2 &lt;- train(form = Sale_Price ~ Gr_Liv_Area + Year_Built, 
  data = ames_train, 
  method = "lm",
  trControl = trainControl(method = "cv", number = 10)
))
```

```
## Linear Regression 
## 
## 2053 samples
##    2 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1846, 1848, 1848, 1848, 1848, 1848, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   46292.38  0.6703298  32246.86
## 
## Tuning parameter 'intercept' was held constant at a value of TRUE
```

---

### Out of sample performance 


```
## 
## Call:
## summary.resamples(object = resamples(list(model1 = cv_model1, model2
##  = cv_model2)))
## 
## Models: model1, model2 
## Number of resamples: 10 
## 
## MAE 
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## model1 34457.58 36323.74 38943.81 39169.09 41660.81 45005.17    0
## model2 28094.79 30594.47 31959.30 32246.86 34210.70 37441.82    0
## 
## RMSE 
##            Min.  1st Qu.   Median     Mean  3rd Qu.     Max. NA's
## model1 47211.34 52363.41 54948.96 56410.89 60672.31 67679.05    0
## model2 37698.17 42607.11 45407.14 46292.38 49668.59 54692.06    0
## 
## Rsquared 
##             Min.   1st Qu.    Median      Mean   3rd Qu.      Max. NA's
## model1 0.3598237 0.4550791 0.5289068 0.5069425 0.5619841 0.5965793    0
## model2 0.5714665 0.6392504 0.6800818 0.6703298 0.7067458 0.7348562    0
```

---

## Regularised regression

Data typically have large number of features. 

As number of features grow models tend to **overfit the data**. 

Regularisation methods provide a means to constrain / regularise the estimated coefficients. 

Easiest way to understand is to see how and why it is applied to OLS. 

Objective in OLS is to find *hyperplane* that minimises SSE between observed and predicted response values. 

This means identifying hyperplane that minimises grey lines in next slide.

---

&lt;img src="lecture-2-slides_files/figure-html/regression_3-1.png" width="720" style="display: block; margin: auto;" /&gt;
---

## Regularised regression

OLS adheres to some specific assumptions

- Linear relationship 
- More observations (n) than features (p) - which means n &gt; p
- No or little multicollinearity

However, for many datasets, such as those involved in text mining, we have more features than observations. 

If we have that p &gt; n there are many potential problems

- Model is less interpretable
- Infinite solutions to OLS problem

Make an assumption that small subset of these features exhibit strongest effect

This is called the **sparsity principle** 

---

## Regularised regression

Objective function of regularised regression model includes penalty parameter `\(P\)`. 

`$$\min(\text{SSE + P})$$`
Penalty parameter constrains the size of coefficients.

Coefficients can only increase is if we have decrease in SSE (i.e. the loss function).

Generalises to other linear models as well (such as logistic regression).

Three common penalty parameters

1. Ridge
2. Lasso [**think cowboys!**]
3. Elastic net (combination of ridge and lasso)I

---

## Ridge regression

Ridge regression controls coefficients by adding `\(\lambda \sum_{j=1}^{p}\beta^{2}_{j}\)` to objective function:

`$$\min(\text{SSE} + \lambda \sum_{j=1}^{p}\beta^{2}_{j})$$`
Size of the penalty is referred to as the `\(L^2\)` norm, or Euclidean norm. 

This penalty can take on different values depending on the tuning parameter `\(\lambda\)`.

If `\(\lambda = 0\)`, then we have OLS regression.

As `\(\lambda \rightarrow \infty\)` the penalty becomes large and forces coefficients to zero. 

**Important to note**: Does not force all the way to zero, only asymptotically. 


---


    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
